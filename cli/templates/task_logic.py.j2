{# templates/task_logic.py #}

import time
import datetime
from typing import List
from pyspark.sql.functions import concat, col, lit, unix_timestamp
from driver.common import find_dataset_by_id
from driver.task_executor import DataSet
from pyspark.sql import SparkSession, Row


def execute(inp_dfs: List[DataSet], spark_session: SparkSession, {% if params is defined and params %}{{ params | join(', ') }}{% endif %}) -> List[DataSet]:
    {% for input in inputs %}
    {{ input.split('.')| last }}_ds = find_dataset_by_id(inp_dfs, '{{ input }}')
    {% endfor %}
    # example of custom logic, adding a new timestamp column to the processed data
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    {{ inputs[0].split('.') | last }}_ds.df = ds.df.withColumn('time', unix_timestamp(lit(timestamp), 'yyyy-MM-dd HH:mm:ss').cast("timestamp"))

    {% for output in outputs %}
    {{ output }}_output = DataSet(id='{{ output }}', df={{ inputs[0].split('.') | last }}_ds.df)
    {% endfor %}
    return [{{ outputs|map('strformat', '%s_output') |join(', ') }}]

