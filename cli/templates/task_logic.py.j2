{# templates/task_logic.py #}

import time
import datetime
from typing import List
from pyspark.sql.functions import concat, col, lit, unix_timestamp
from driver.util import filter_list_by_id
from driver.task_executor import DataSet
from pyspark.sql import SparkSession, Row


def execute(inp_dfs: List[DataSet], spark_session: SparkSession, {% if params is defined and params %}{{ params |map('tuple_mapper', '=') | join(', ') }}{% endif %}) -> List[DataSet]:
    {% for input in inputs %}
    {{ input.split('.')| last }}_ds = filter_list_by_id(inp_dfs, '{{ input }}')
    {% endfor %}
    # example of custom logic, adding a new timestamp column to the processed data
    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
    data_frame = {{ inputs[0].split('.') | last }}_ds.df
    data_frame = data_frame.withColumn('time', unix_timestamp(lit(timestamp), 'yyyy-MM-dd HH:mm:ss').cast("timestamp"))

    {% for output in outputs %}
    {{ output.split('.')[-1] }}_output = DataSet(model_id='{{ output.split('.')[-1] }}', df=data_frame)
    {% endfor %}
    return [{{ outputs|map('extract_table')|map('strformat', '%s_output') |join(', ') }}]

