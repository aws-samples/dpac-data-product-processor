{% set input_models = inputs|map('split_and_pick',-1) %}

import os
import driver
import tasks.{{ task_name }}
from typing import List
from driver.core import ConfigContainer, resolve_data_set_id
from pyspark.sql import DataFrame
from driver import DataSet
from driver.processors import schema_checker, constraint_processor, transformer_processor
from test_config import {{ input_models| map('strformat', '%s_df') |join(', ') }}, {{ models | map(attribute='id', default='unavailable') | map('strformat', '%s_schema') |join(', ') }}, app_args

def test_{{ task_name }}(spark_session, {{ input_models| map('strformat', '%s_df: DataFrame') |join(', ') }}):
    inputs = list()
    {% for input in inputs %}
    {{ input.split('.')[-1] }}_ds = DataSet(model_id='{{ input }}', df={{ input.split('.')[-1] }}_df)
    inputs.append({{ input.split('.')[-1] }}_ds)
    {% endfor %}
    outputs: List[DataSet] = tasks.{{ task_name }}.execute(inputs, spark_session, {% if params is defined and params %}{{ params |map('tuple_mapper', '=') | join(', ') }}{% endif %})
    for dataset in outputs:
        assert dataset.id in [{{ outputs|map('strformat', '"%s"') |join(', ') }}]
        # Here you can provide additional assertions
        # assert input_ds.df.count() == output_ds.df.count()
        dataset.df.show()
        dataset.df.describe()


def test_end_to_end(spark_session, spark_context, {{ inputs| map('split', '.')|list|last |map('strformat', '%s_df: DataFrame') |join(', ') }}, app_args):
    current_folder = os.path.join(os.path.dirname(os.path.realpath(__file__)), '..')
    print(f'End to end test executed in working folder: {current_folder}')

    {% if inputs is defined %}
    def mock_input_handler(input_definition: ConfigContainer):
        dfs = {
            {% for input in inputs %}
            "{{ input }} ": {{ input.split('.')[-1] }}_df
            {% endfor %}
            }
        return dfs.get(resolve_data_set_id(input_definition))
    {% endif %}

    {% if outputs is defined %}
    def mock_output_handler(dataset: DataSet):
        assert dataset.id in [{{ outputs|map('strformat', '"%s"') |join(', ') }}]
        # Here you can provide the assertions specifc to the output
        # assert dataset.df.count() == 30
        dataset.df.show()
        dataset.df.describe()
    {% endif %}

    driver.init(spark_session)
    {% if inputs is defined %}
    driver.register_data_source_handler('connection', mock_input_handler)
    driver.register_data_source_handler('model', mock_input_handler)
    driver.register_data_source_handler('file', mock_input_handler)
    {% endif %}
    driver.register_postprocessors(transformer_processor, schema_checker, constraint_processor)
    driver.register_output_handler('default', mock_output_handler)
    {% if outputs is defined %}
    driver.register_output_handler('lake', mock_output_handler)
    {% endif %}
    driver.process_product(app_args, current_folder)
