stages:
  - test
  - package
  - deploy

test_job_interpreter:
  stage: test
  image:
    name: registry.gitlab.aws.dev/aws-sa-dach/teams/dnb/docker-glue-pyspark:1.2.1
    entrypoint:  [ "/bin/bash"]
  script:
    - pip install --upgrade pip
    - pip install -U -e  .
    - pip install -r requirements-test.txt
    - pytest --cov=deprecated -s -m 'not integration'

package_job_interpreter:
  stage: package
  image:
    name: registry.gitlab.aws.dev/aws-sa-dach/teams/dnb/docker-glue-pyspark:1.2.1
    entrypoint:  ["/bin/bash"]
  script:
    - python -m pip install --upgrade pip
    - pip install -U -e  .
    - pip install -r requirements-test.txt
    - python setup.py build -vf && python setup.py bdist_wheel
  artifacts:
    paths:
      - ./dist/*
      - ./main.py
    expire_in: 30 days

deploy_job_interpreter:
  stage: deploy
  image: amazon/aws-cli:latest
  script:
    - yum install -y zip
    - mkdir package && cp ./dist/* ./package && cp ./main.py ./package
    - cd package && zip -r ../data-product-processor.zip ./* && cd -
    - aws s3 cp ./data-product-processor.zip s3://dpac-solution-artefacts/main.py
